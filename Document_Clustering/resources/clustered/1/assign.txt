CSE471: Statistical Methods in AI - Spring 2015 Assignment 3: PERCEPTRON CLASSIFIER DUE: Before 5:00pm on 16 March 2015 (Monday) INSTRUCTIONS: 1. You may do the assignment in Matlab/Octave, R, Python, C/C++ or Java. 2. You need to upload a single pdf file in the Courses Portal. The file should contain your answers as well as the code you have written and its output. 3. At the topright of the first page of your submission, include the assignment number, your name and roll number. 4. IMPORTANT: Make sure that the assignment that you submit is your own work. Do not copy any part from any source including your friends, seniors or the internet. Any breach of this rule could result in serious actions including an F grade in the course. Also please submit valid reports – we noticed people submitting empty files, no results, etc! 5. Your grade will depend on the correctness of answers and output. In addition, due consideration will be given to the clarity and details of your answers and the legibility and structure of your code. Preamble: The idea of this assignment is to explore the material presented as part of Chapter 5 on constructing Perceptronbased Linear Discriminant Functions. The data set to be used for the exercises given below is the following sample set comprising a twoclass problem. 1 = [(1; 6); (7; 2); (8; 9); (9; 9); (4; 8); (8; 5)] 2 = [(2; 1); (3; 3); (2; 4); (7; 1); (1; 3); (5; 2)] Problems: Implement the following algorithms i. Singlesample perceptron ii. Singlesample perceptron with margin iii. Relaxation algorithm with margin iv. WidrowHoff or Least Mean Squared (LMS) Rule 1. In each case, plot the data points in a graph (e.g. red: class1 and blue: class2) and also show the weight vector a learnt from all of the above algorithms in the same graph (labeling clearly to distinguish different solutions). 2. Create a test set comprising three more data samples (yi) for each class and test your implementation by computing for each of test samples the output predicted by the respective algorithm (atyi). Create a comparison table listing test set accuracies of each of the above algorithms. 3. Run each of the above algorithms for various initial values of the weight vector, and comment on the dependence of convergence time (runtime) on initialization. 4. Similarly explore the effect of adding different margins on the final solution as well as on the convergence (run) time for algorithms (ii) and (iii). 5. The report should clearly detail the formulation of the problem (construction of augmented features, weight vector notation, etc.) and specifically list the test set that you created and the results thereof. 6. As part of the submission include the code for each of the algorithms along with a small report that explains the algorithms, implementation details, the results and their analysis. Bonus Points: 7. Remember the discussion on LMS and Perceptron solutions being different in some cases – can you experiment and construct a training set that makes the solution of LMS rule differ from those of the other. Of course if they are already different, can you adjust the dataset so that the solutions align! 8. Modify the dataset such that it becomes linearly nonseparable (clearly list the changes in the report). Now run algorithms (iii) and (iv) with suitable stopping criteria. Plot the data points in a graph (e.g. red: class1 and blue: class2) and also show the weight vector a learnt from these two algorithms in the same graph (labeling clearly to distinguish different solutions) and comment on the nature of the solution found in each case. -xxxxxxxx 